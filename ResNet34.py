import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms, models
from torchvision.models import ResNet34_Weights
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è·¯å¾„å’Œè¶…å‚æ•°
data_dir = 'chest_xray_lung'
batch_size = 32
num_epochs = 20
learning_rate = 0.00005

# æ•°æ®å¢å¼º
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomAffine(translate=(0.05, 0.05), degrees=0),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])
}


def load_data():
    image_datasets = {
        x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])
        for x in ['train', 'val', 'test']
    }

    dataloaders = {
        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0)
        for x in ['train', 'val', 'test']
    }

    return image_datasets, dataloaders


def compute_class_weights(dataset):
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    labels = [label for _, label in dataset]
    labels_tensor = torch.tensor(labels)
    class_counts = torch.bincount(labels_tensor)
    class_weights = 1.0 / class_counts.float()
    class_weights = class_weights / class_weights.sum() * len(class_counts)  # å½’ä¸€åŒ–ï¼Œä¿æŒæ•°é‡æ„ŸçŸ¥

    print("\nğŸ“Š Class counts:", class_counts.tolist())
    print("âš–ï¸  Class weights:", class_weights.tolist())
    return class_weights.to(device)


def train_model(model, dataloaders, dataset_sizes, class_weights):
    # ä½¿ç”¨åŠ æƒäº¤å‰ç†µæŸå¤±
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print("-" * 20)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloaders[phase]:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            print(f"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")


def evaluate_model(model, dataloader, class_names):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            y_true.extend(labels.tolist())
            y_pred.extend(preds.cpu().tolist())

    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ Classification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # æ··æ·†çŸ©é˜µå¯è§†åŒ–
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()


def main():
    # åŠ è½½æ•°æ®
    image_datasets, dataloaders = load_data()
    class_names = image_datasets['train'].classes
    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}

    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = compute_class_weights(image_datasets['train'])

    # åŠ è½½æ¨¡å‹
    model = models.resnet34(weights=ResNet34_Weights.DEFAULT)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, 3)
    model = model.to(device)

    # è®­ç»ƒ
    train_model(model, dataloaders, dataset_sizes, class_weights)

    # æµ‹è¯• + è¯„ä¼°
    evaluate_model(model, dataloaders['test'], class_names)


if __name__ == "__main__":
    main()
